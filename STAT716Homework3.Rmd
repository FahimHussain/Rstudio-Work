---
title: "716 Homework 3"
author: "Fahim Hussain"
date: "September 20, 2018"
output:
  pdf_document: default
  html_document: default
---
```{r include = FALSE}
library(ggplot2)
```

###Question 13

#####a)Using the rnorm() function, create a vector, x, containing 100 observations drawn from a N(0, 1) distribution. This represents a feature, X.

```{r}
set.seed(1)
x <- rnorm(100)
x
```

#####b) Using the rnorm() function, create a vector, eps, containing 100 observations drawn from a N(0, 0.25) distribution i.e. a normal distribution with mean zero and variance 0.25.

```{r}
eps <- rnorm(100, sd=0.25)
eps
```

#####c) ) Using x and eps, generate a vector y according to the model Y = -1 +0.5X + \(\epsilon\). What is the length of the vector y? What are the values of 	$\beta_{0}$ and $\beta_{1}$ in this linear model?

```{r}
y <- -1 + 0.5*x + eps
length(y)
```

#####The length of y is 100.	$\beta_{0}$ is -1 and 	$\beta_{1}$ is 0.5

#####Create a scatterplot displaying the relationship between x and y. Comment on what you observe.

```{r}
reg <- data.frame(x,y)
ggplot(reg, aes(x,y))+geom_point()
```

#####We can see that the relationship between x and y seems to be linear in an upward trend. There seems to be some scatter and deviation from the true line, which could be due to the error term.

#####e)Fit a least squares linear model to predict y using x. Comment on the model obtained. How do  $\widehat{\beta_{0}}$ and $\widehat{\beta_{1}}$ compare to $\beta_{0}$ and $\beta_{1}$?

```{r}
lm_fit <- lm(y~x)
summary(lm_fit)

```

##### The values of  $\widehat{\beta_{0}}$ and $\widehat{\beta_{1}}$ are similar to the values of $\beta_{0}$ and $\beta_{1}$. I get -1.00942 and 0.49973 for each betas, respectively. This is extremely close to the -1 and 0.5 from the betas of the original equation. The p values are also close to 0, which means that I can reject the null and conclude that my betas are not 0. The F-statistic is also a high value, which shows that I have significant coefficients.

#####f)Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend.

```{r}
plot(x,y, pch=21, bg="black")
abline(lm_fit, col="red")
abline(-1,0.5, col="blue")
legend("bottomright", c("Least Squares Line", "Pop Regression Line"), col=c("red", "blue"), lty = c(1,1))
```

#####g) Now fit a polynomial regression model that predicts y using x and x2. Is there evidence that the quadratic term improves the model fit? Explain your answer.

```{r}
lm_fit2 <- lm(y~x+I(x^2))
summary(lm_fit2)
```

#####The p value suggest that the quadratic term is not useful. The R-squared value increased but only by a small amount and the F-statistic value dropped. This shows that the extra quadratic term does not improve the model fit.

#####h)Repeat (a)-(f) after modifying the data generation process in such a way that there is less noise in the data. The initial model should remain the same. Describe your results.

#####In order to reduce the noise, I will lower the value of the variance of the error term.

```{r}
set.seed(2)
x<-rnorm(100)
eps <- rnorm(100,sd=0.1)
y<- -1 + 0.5*x + eps
reg <- data.frame(x,y)
ggplot(reg, aes(x,y))+geom_point()
lm_fit3 <- lm(y~x)
summary(lm_fit3)
```

##### Because there is not much variance in the error term, the dispersion is minimized. This shows us a linear model that is almost perfectly correlated. The betas for the model is almost identical to the population model, with an incredibly high R-squared and F-statistic value.
```{r}
plot(x,y, pch=21, bg="black")
abline(lm_fit3, col="red")
abline(-1,0.5, col="blue")
legend("bottomright", c("Least Squares Line", "Pop Regression Line"), col=c("red", "blue"), lty = c(1,1))
```

#####We see that the lines almost sit on top of each other, as expected.

#####i) Repeat (a)-(f) after modifying the data generation process in such a way that there is more noise in the data. The model (3.39) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term  in (b). Describe your results

#####Instead of reducing the variance, I will increase the volume to increase the noise in the data.

```{r}
set.seed(3)
x<-rnorm(100)
eps <- rnorm(100,sd=0.6)
y<- -1 + 0.5*x + eps
reg <- data.frame(x,y)
ggplot(reg, aes(x,y))+geom_point()
lm_fit4 <- lm(y~x)
summary(lm_fit3)
```


#####We see that the points are more dispersed out than the first plot I created. The R-squared has reduced and the residual standard error has increased. Our beta coefficients are this close to the true value, although a little more off than the other models.

```{r}
plot(x,y, pch=21, bg="black")
abline(lm_fit4, col="red")
abline(-1,0.5, col="blue")
legend("bottomright", c("Least Squares Line", "Pop Regression Line"), col=c("red", "blue"), lty = c(1,1))
```

#####We can see a clear difference in this graph than from the other graphs. The fit has a noticeable smaller slope than the actual regression line but it has a higher intercept.


#####j)What are the confidence intervals for $\beta_{0}$ and $\beta_{1}$ based on the original data set, the noisier data set, and the less noisy data set? Comment on your results.

```{r}
confint(lm_fit)
confint(lm_fit3)
confint(lm_fit4)
```

#####It seems as though the confidence intervals' width increases as the noise increases. However, all the intervals seem to center around the true slope and true intercept.
